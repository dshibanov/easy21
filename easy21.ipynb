{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy21 assignment\n",
    "from UCL Course on RL https://www.davidsilver.uk/teaching/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  Implementation of Easy21 (10 marks)\n",
    "\n",
    "You should write an environment that implements the game Easy21.  \n",
    "Specifically, write a function, namedstep, which takes as input a states (dealer’s firstcard 1–10 and the player’s sum 1–21), and an action $a$ (hit or stick), and returns a sample of the next state $s^′$(which may be terminal if the game is finished) and reward $r$.  We will be using this environment for model-free reinforcement learning, and you should not explicitly represent the transition matrix for the MDP. There is no discounting $(γ= 1)$.  You should treat the dealer’s moves as part of the environment, i.e.  calling stepwith as tick action will play out the dealer’s cards and return the final reward and terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2  Monte-Carlo Control in Easy21 (15 marks)\n",
    "\n",
    "Apply Monte-Carlo control to Easy21.  \n",
    "* Initialise the value function to zero.  \n",
    "* Use a time-varying scalar step-size of $\\alpha_t = {1}/{N(s_t, a_t)}$  and an $\\epsilon$\u000f-greedy exploration strategy  with $\\epsilon = N_0/(N_0+N(s_t))$,  where $N_0 =  100$  is  a  constant, $N(s)$  is the number of times that states has been visited, and $N(s,a)$ is the number of times that action $a$ has been selected from states.  \n",
    "* Feel free to choose an alternative value for $N_0$, if it helps producing better results. \n",
    "* Plot the optimal value function $V^∗(s) = max_a{Q^∗(s,a)}$ using similar axes to the following figure taken from Sutton and Barto’s Blackjack example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 TD Learning in Easy21 (15 marks)\n",
    "\n",
    "* Implement $Sarsa(λ)$ in 21s.  \n",
    "* Initialise the value function to zero.  \n",
    "* Use the samestep-size and exploration schedules as in the previous section. \n",
    "* Run the algorithmwith parameter values $\\lambda \\in \\{ 0,0.1,0.2 , \\ldots , 1 \\}$.  \n",
    "* Stop each run after 1000 episodes and report the mean-squared error $\\sum _ { s , a } ( Q ( s , a ) - Q ^ { * } ( s , a ) ) ^ { 2 }$ over all states $s$ and  actions $a$,  comparing  the  true  values $Q^∗(s,a)$  computed  in  the  previous section with the estimated values $Q(s,a)$ computed by Sarsa.  \n",
    "* Plot the mean-squared error against $\\lambda$.  \n",
    "* For $\\lambda = 0$ and $\\lambda = 1$ only, plot the learning curve ofmean-squared error against episode number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Linear Function Approximation in Easy21 (15 marks)\n",
    "\n",
    "We now consider a simple value function approximator using coarse coding.  \n",
    "* Use a binary feature vector $\\varphi(s,a)$ with 3∗6∗2 = 36 features.  \n",
    "* Each binary feature has a value of 1 iff (s,a) lies within the cuboid of state-space corresponding to that feature, and the action corresponding to that feature.  The cuboids have the following overlapping intervals:\n",
    "\n",
    "$ dealer(s) ={[1,4],[4,7],[7,10]}$\n",
    "\n",
    "$ player(s) ={[1,6],[4,9],[7,12],[10,15],[13,18],[16,21]}$ \n",
    "\n",
    "$a={hit,stick}$\n",
    "\n",
    "where\n",
    "    * dealer(s) is the value of the dealer’s first card (1–10)\n",
    "    * sum(s) is the sum of the player’s cards (1–21)\n",
    "    \n",
    "* Repeat the $Sarsa(λ)$ experiment from the previous section, but using linear value function approximation \n",
    "$Q ( s , a ) = \\phi ( s , a ) ^ { T } \\theta$.  Use a constant exploration of $\\epsilon = 0.05$ and a constant step-size of 0.01.  \n",
    "* Plot the mean-squared error againstλ.  For $\\lambda= 0$ and $\\lambda = 1$ only,  plot the learning curve of mean-squared erroragainst episode number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  Discussion\n",
    "\n",
    "Discuss the choice of algorithm used in the previous section.\n",
    "•What are the pros and cons of bootstrapping in Easy21?\n",
    "•Would  you  expect  bootstrapping  to  help  more  in blackjack or Easy21? Why?\n",
    "•What are the pros and cons of function approximation in Easy21?\n",
    "•How would you modify the function approximator suggested in this sectionto get better results in Easy21?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel377",
   "language": "python",
   "name": "kernel377"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
