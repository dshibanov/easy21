{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy21 assignment \n",
    "from UCL Course on RL https://www.davidsilver.uk/teaching/\n",
    "\n",
    "full description https://www.davidsilver.uk/wp-content/uploads/2020/03/Easy21-Johannes.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  Implementation of Easy21 (10 marks)\n",
    "\n",
    "You should write an environment that implements the game Easy21.\n",
    "\n",
    "Specifically, write a function, namedstep, which takes as input a states (dealer’s firstcard 1–10 and the player’s sum 1–21), and an action $a$ (hit or stick), and returns a sample of the next state $s^′$(which may be terminal if the game is finished) and reward $r$.  We will be using this environment for model-free reinforcement learning, and you should not explicitly represent the transition matrix for the MDP. There is no discounting $(γ= 1)$.  You should treat the dealer’s moves as part of the environment, i.e.  calling step with a stick action will play out the dealer’s cards and return the final reward and terminal state.\n",
    "\n",
    "1. first step two black cards to everybody\n",
    "2. then player is turning\n",
    "3. state = {dealer_sum, player_sum}\n",
    "4. player does actions {hit,stick}\n",
    "5. stick - no new card, hit - a new one\n",
    "6. if player.sum() > 21 | < 1 then 'goes bust', return reward -1\n",
    "7. if player stick the dealer starts making turns\n",
    "8. the dealer always sticks on any sum of >=17 and hits otherwise\n",
    "9. red card subtract value from player.sum, black card add\n",
    "10. if the dealer 'goes bust' reward +1, if player -1, if player has 21 reward is 0\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "code_folding": [
     3,
     8,
     16,
     19,
     22,
     26,
     31
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Easy21:\n",
    "    def __init__(self):        \n",
    "        self.dealer_sum = 0\n",
    "        self.player_sum = 0\n",
    "        self.started = False\n",
    "                    \n",
    "    def draw_a_card(self, black_only=False):                        \n",
    "        if black_only: \n",
    "            r = random.randint(1, 10)                \n",
    "        else:\n",
    "            r = random.randint(1, 10)*(-1 if random.random() < 1/3 else 1)\n",
    "        print('C:', r)\n",
    "        return r\n",
    "    \n",
    "    def state(self):\n",
    "            return [self.dealer_sum, self.player_sum]\n",
    "        \n",
    "    def draw(self):\n",
    "        return self.player_sum == 21\n",
    "    \n",
    "    def goes_bust(self, summ):        \n",
    "        if summ > 21 or summ < 1:            \n",
    "            return True\n",
    "        \n",
    "    def dealer_hit(self):\n",
    "        if self.dealer_sum < 17:\n",
    "            return True\n",
    "        return False\n",
    "                \n",
    "    def step(self, state, action): \n",
    "        if self.started == False:            \n",
    "            self.player_sum = self.draw_a_card(True)\n",
    "            self.dealer_sum = self.draw_a_card(True)\n",
    "            self.started = True\n",
    "            return self.state()\n",
    "        else:\n",
    "            if action == 'hit':\n",
    "                self.player_sum += self.draw_a_card()\n",
    "                if self.draw():\n",
    "                    self.started = False\n",
    "                    return 0\n",
    "                if self.goes_bust(self.player_sum):\n",
    "                    self.started = False\n",
    "                    return -1\n",
    "                return self.state()\n",
    "            else:             \n",
    "                while self.dealer_hit():\n",
    "                    self.dealer_sum += self.draw_a_card()\n",
    "                    if self.goes_bust(self.dealer_sum):\n",
    "                        self.started = False\n",
    "                        return 1\n",
    "                if self.dealer_sum > self.player_sum:\n",
    "                    return -1            \n",
    "                return self.state()\n",
    "\n",
    "e = Easy21() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 4\n",
      "C: 4\n",
      "[4, 4]\n"
     ]
    }
   ],
   "source": [
    "print(e.step(e.state(), 'hit'))\n",
    "# print(e.state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 3\n",
      "C: 8\n",
      "C: -7\n",
      "C: 8\n",
      "C: -4\n",
      "C: 10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(e.step(e.state(), 'stick'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2  Monte-Carlo Control in Easy21 (15 marks)\n",
    "\n",
    "Apply Monte-Carlo control to Easy21.  \n",
    "* Initialise the value function to zero.  \n",
    "* Use a time-varying scalar step-size of $\\alpha_t = {1}/{N(s_t, a_t)}$  and an $\\epsilon$\u000f-greedy exploration strategy  with $\\epsilon = N_0/(N_0+N(s_t))$,  where $N_0 =  100$  is  a  constant, $N(s)$  is the number of times that states has been visited, and $N(s,a)$ is the number of times that action $a$ has been selected from states.  \n",
    "* Feel free to choose an alternative value for $N_0$, if it helps producing better results. \n",
    "* Plot the optimal value function $V^∗(s) = max_a{Q^∗(s,a)}$ using similar axes to the following figure taken from Sutton and Barto’s Blackjack example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 TD Learning in Easy21 (15 marks)\n",
    "\n",
    "* Implement $Sarsa(λ)$ in 21s.  \n",
    "* Initialise the value function to zero.  \n",
    "* Use the samestep-size and exploration schedules as in the previous section. \n",
    "* Run the algorithmwith parameter values $\\lambda \\in \\{ 0,0.1,0.2 , \\ldots , 1 \\}$.  \n",
    "* Stop each run after 1000 episodes and report the mean-squared error $\\sum _ { s , a } ( Q ( s , a ) - Q ^ { * } ( s , a ) ) ^ { 2 }$ over all states $s$ and  actions $a$,  comparing  the  true  values $Q^∗(s,a)$  computed  in  the  previous section with the estimated values $Q(s,a)$ computed by Sarsa.  \n",
    "* Plot the mean-squared error against $\\lambda$.  \n",
    "* For $\\lambda = 0$ and $\\lambda = 1$ only, plot the learning curve ofmean-squared error against episode number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Linear Function Approximation in Easy21 (15 marks)\n",
    "\n",
    "We now consider a simple value function approximator using coarse coding.  \n",
    "* Use a binary feature vector $\\varphi(s,a)$ with 3∗6∗2 = 36 features.  \n",
    "* Each binary feature has a value of 1 iff (s,a) lies within the cuboid of state-space corresponding to that feature, and the action corresponding to that feature.  The cuboids have the following overlapping intervals:\n",
    "\n",
    "$ dealer(s) ={[1,4],[4,7],[7,10]}$\n",
    "\n",
    "$ player(s) ={[1,6],[4,9],[7,12],[10,15],[13,18],[16,21]}$ \n",
    "\n",
    "$a={hit,stick}$\n",
    "\n",
    "where\n",
    "    * dealer(s) is the value of the dealer’s first card (1–10)\n",
    "    * sum(s) is the sum of the player’s cards (1–21)\n",
    "    \n",
    "* Repeat the $Sarsa(λ)$ experiment from the previous section, but using linear value function approximation \n",
    "$Q ( s , a ) = \\phi ( s , a ) ^ { T } \\theta$.  Use a constant exploration of $\\epsilon = 0.05$ and a constant step-size of 0.01.  \n",
    "* Plot the mean-squared error againstλ.  For $\\lambda= 0$ and $\\lambda = 1$ only,  plot the learning curve of mean-squared erroragainst episode number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  Discussion\n",
    "\n",
    "Discuss the choice of algorithm used in the previous section.\n",
    "•What are the pros and cons of bootstrapping in Easy21?\n",
    "•Would  you  expect  bootstrapping  to  help  more  in blackjack or Easy21? Why?\n",
    "•What are the pros and cons of function approximation in Easy21?\n",
    "•How would you modify the function approximator suggested in this sectionto get better results in Easy21?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel377",
   "language": "python",
   "name": "kernel377"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
